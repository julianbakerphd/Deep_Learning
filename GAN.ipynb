{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import tensorflow\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Input, Dense, Conv2D, BatchNormalization, Dropout, Flatten\n",
    "from tensorflow.keras.layers import Activation, Reshape, Conv2DTranspose, UpSampling2D\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_images = \"quickdraw_data/apple.npy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load(input_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(144722, 784)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(144722, 28, 28, 1)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data/255\n",
    "data = np.reshape(data, (data.shape[0], 28,28,1))\n",
    "img_w, img_h = data.shape[1:3]\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f9eb35d41d0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(data[4343,:,:,0], cmap='Greys')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_builder(width=64,p=0.4):\n",
    "    \n",
    "    # define inputs:\n",
    "    inputs = Input((img_w, img_h, 1))\n",
    "    \n",
    "    # convolutional layers\n",
    "    conv1 = Conv2D(width*1, 5, strides=2, padding='same', activation='relu')(inputs)\n",
    "    conv1 = Dropout(p)(conv1)\n",
    "    \n",
    "    conv2 = Conv2D(width*2, 5, strides=2, padding='same', activation='relu')(conv1)\n",
    "    conv2 = Dropout(p)(conv2)\n",
    "    \n",
    "    conv3 = Conv2D(width*4, 5, strides=2, padding='same', activation='relu')(conv2)\n",
    "    conv3 = Dropout(p)(conv3)\n",
    "    \n",
    "    conv4 = Conv2D(width*8, 5, strides=1, padding='same', activation='relu')(conv3)\n",
    "    conv4 = Flatten()(Dropout(p)(conv4))\n",
    "\n",
    "    # output layer\n",
    "    output = Dense(1, activation='sigmoid')(conv4)\n",
    "    \n",
    "    # model definition\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=output)\n",
    "    model.summary()\n",
    "    \n",
    "    return model    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 28, 28, 1)]       0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 14, 14, 64)        1664      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 7, 7, 128)         204928    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 4, 4, 256)         819456    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 4, 4, 512)         3277312   \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 8193      \n",
      "=================================================================\n",
      "Total params: 4,311,553\n",
      "Trainable params: 4,311,553\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "discriminator = discriminator_builder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator.compile(loss='binary_crossentropy', optimizer=RMSprop(lr=0.0008, decay=6e-8, clipvalue=1.0), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create generator network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_builder(z_dim=100, width=64, p=0.4):\n",
    "    \n",
    "    # define inputs\n",
    "    inputs = Input((z_dim,))\n",
    "    \n",
    "    # first dense layer\n",
    "    dense1 = Dense(7*7*64)(inputs)\n",
    "    dense1 = BatchNormalization(momentum=0.9)(dense1)\n",
    "    dense1 = Activation(activation='relu')(dense1)\n",
    "    dense1 = Reshape((7,7,64))(dense1)\n",
    "    dense1 = Dropout(p)(dense1)\n",
    "    \n",
    "    # deconvolutional layer\n",
    "    \n",
    "    conv1 = UpSampling2D()(dense1)\n",
    "    conv1 = Conv2DTranspose(int(width/2), kernel_size=5, padding='same', activation=None)(conv1)\n",
    "    conv1 = BatchNormalization(momentum=0.9)(conv1)\n",
    "    conv1 = Activation(activation='relu')(conv1)\n",
    "  \n",
    "    conv2 = UpSampling2D()(conv1)\n",
    "    conv2 = Conv2DTranspose(int(width/4), kernel_size=5, padding='same', activation=None)(conv2)\n",
    "    conv2 = BatchNormalization(momentum=0.9)(conv2)\n",
    "    conv2 = Activation(activation='relu')(conv2)\n",
    "    \n",
    "    conv3 = Conv2DTranspose(int(width/8), kernel_size=5, padding='same', activation=None)(conv2)\n",
    "    conv3 = BatchNormalization(momentum=0.9)(conv3)\n",
    "    conv3 = Activation(activation='relu')(conv3) \n",
    "    \n",
    "    # output layer\n",
    "    output = Conv2D(1, kernel_size=5, padding='same', activation='sigmoid')(conv3)\n",
    "    \n",
    "    # define model\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=output)\n",
    "    model.summary()\n",
    "    \n",
    "    return model   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         [(None, 100)]             0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 3136)              316736    \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 3136)              12544     \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "reshape_2 (Reshape)          (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_3 (UpSampling2 (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTr (None, 14, 14, 32)        51232     \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 14, 14, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_4 (UpSampling2 (None, 28, 28, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_4 (Conv2DTr (None, 28, 28, 16)        12816     \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 28, 28, 16)        64        \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 28, 28, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_5 (Conv2DTr (None, 28, 28, 8)         3208      \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 28, 28, 8)         32        \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 28, 28, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 28, 28, 1)         201       \n",
      "=================================================================\n",
      "Total params: 396,961\n",
      "Trainable params: 390,577\n",
      "Non-trainable params: 6,384\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "generator = generator_builder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Create adversarial network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adversarial_builder(z_dim=100):\n",
    "    model = Sequential()\n",
    "    model.add(generator)\n",
    "    model.add(discriminator)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=RMSprop(lr=0.00004,decay=3e-8, clipvalue=1.0),\n",
    "                  metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "functional_5 (Functional)    (None, 28, 28, 1)         396961    \n",
      "_________________________________________________________________\n",
      "functional_1 (Functional)    (None, 1)                 4311553   \n",
      "=================================================================\n",
      "Total params: 4,708,514\n",
      "Trainable params: 4,702,130\n",
      "Non-trainable params: 6,384\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "adversarial_model = adversarial_builder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_trainable(net, val):\n",
    "    net.trainable = val\n",
    "    for l in net.layers:\n",
    "        l.trainable = val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs=2000, batch=128):\n",
    "    d_metrics = []\n",
    "    a_metrics = []\n",
    "    \n",
    "    running_d_loss = 0\n",
    "    running_d_acc = 0\n",
    "    running_a_loss = 0\n",
    "    running_a_acc = 0\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        \n",
    "        if i%100 == 0:\n",
    "            print(i)\n",
    "        real_imgs = np.reshape(data[np.randpm.choice(data.shape[0], batch, replace=False)], (batch(28,28,1)))\n",
    "        fake_imgs = generator.predict(np.random.uniform(-1.0,1.0, size=[batch, 100]))\n",
    "        \n",
    "        x = np.concatenate((real_imgs, fake_imgs))\n",
    "        y = np.ones([2*batch,1])\n",
    "        y[batch:,:] = 0\n",
    "        \n",
    "        make_trainable(discriminator, True)\n",
    "        \n",
    "        d_meterics.append(discriminator.train_on_batch(x,y))\n",
    "        running_d_loss += d_metrics[-1][0]\n",
    "        running_d_acc += d_meterics[-1][1]\n",
    "        \n",
    "        make_trainable(discriminator, False)\n",
    "        \n",
    "        noise = np.random.uniform(-1.0,1.0, size=[batch, 100])\n",
    "        y = np.ones([batch,1])\n",
    "  \n",
    "        a_meterics.append(adversarial_model.train_on_batch(noise,y))\n",
    "        running_a_loss += a_metrics[-1][0]\n",
    "        running_a_acc += a_meterics[-1][1]\n",
    "        \n",
    "        if (i+1)%100 == 0:\n",
    "\n",
    "            print('Epoch #{}'.format(i))\n",
    "            log_mesg = \"%d: [D loss: %f, acc: %f]\" % \\\n",
    "            (i, running_d_loss/i, running_d_acc/i)\n",
    "            log_mesg = \"%s  [A loss: %f, acc: %f]\" % \\\n",
    "            (log_mesg, running_a_loss/i, running_a_acc/i)\n",
    "            print(log_mesg)\n",
    "\n",
    "            noise = np.random.uniform(-1.0, 1.0, \n",
    "                                      size=[16, z_dim])\n",
    "            gen_imgs = generator.predict(noise)\n",
    "\n",
    "            plt.figure(figsize=(5,5))\n",
    "\n",
    "            for k in range(gen_imgs.shape[0]):\n",
    "                plt.subplot(4, 4, k+1)\n",
    "                plt.imshow(gen_imgs[k, :, :, 0], \n",
    "                           cmap='gray')\n",
    "                plt.axis('off')\n",
    "                \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "    \n",
    "    return a_metrics, d_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_metrics_complete, d_metrics_complete = train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (py3-TF2.0)",
   "language": "python",
   "name": "py3-tf2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
